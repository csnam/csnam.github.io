<h4>12장 다층 퍼셉트론</h4>

<h4>14장 CNN  이미지 분류</h4>

<p>
Multilayer Perceptron (MLP)
</p>
<p>
-입력층, 은익층, 출력층,
</p>
<p>
 <a href="https://github.com/rickiepark/python-machine-learning-book-2nd-edition">https://github.com/rickiepark/python-machine-learning-book-2nd-edition</a>
</p>
<!--H6 not demoted to H7. -->

<h6>tensorflow 1.0 기본</h7>


<ol>

<li>Graph를 만든다 
<ol>
 
<li>placeholder 만들고 입력파라미터  형태 지정. x
 
<li>Variable w , Variable bias 지정
 
<li>init > variables  초기화
</li> 
</ol>

<li>Session을 만들고  Sess.run 
<ol>
 
<li>sess.run(init)
 
<li>sess.run (  feed_dict={x:t})(
</li> 
</ol>
</li> 
</ol>
<p>
	
</p>

<code>

   <pre class="prettyprint">
      g = tf.Graph()
      with g.as_default():
         x= tf.compat.v1.placeholder(dtype= tf.float32, shape=(None),name='x')
         w = tf.Variable(2.0,name='weight')
         b = tf.Variable(0.7,name='bias')
      
         z = w * x + b
         init = tf.compat.v1.global_variables_initializer()
      
      with tf.compat.v1.Session(graph=g) as sess:
         sess.run(init)
         for t in [1.0,0.6,-1.8]:
                 print('x=%4.1f -> %4.1f'%(
                     t,sess.run(z,feed_dict={x:t})))</pre>
      

</code>



<!--H6 not demoted to H7. -->

<h6>tensorflow 2.0 기본</h7>



<code>
   <pre class="prettyprint">#tensor flow 2.0
      w = tf.Variable(2.0,name='weight')
      b = tf.Variable(0.7,name='bias')
      
      for x in [1.0,0.6,-1.8]:
         z = w * x + b
         print('x=%4.1f -> %4.1f'%(x,z))
      print(z)
      
      # tensor flow 2.0  for 문 제거
      z = w * [1.0,0.6,-1.8] + b
      print(z.numpy())</pre>
      
</code>



<p>
tf.keras가 tensorflow 2.0에서는 표준 파이썬 API
</p>
<p>
<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>
</p>
<h4>minst 예제 </h4>


<ul>

<li>표준화 

    

<pre class="prettyprint">#(Xi - mean(x)) / (std(x))  =>  mean(x): 평균  std(x): 표준편차
X_train_center = (X_train -mean_vals) / std_val
X_test_center =  (X_test - mean_vals) / std_val
</pre>


</li>
</ul>
<ul>

<li>원-핫 인코딩

    

<pre class="prettyprint">y_train_onehot = tf.keras.utils.to_categorical(y_train)</pre>


</li>
</ul>
<ul>

<li>모델 만들기 계층 쌓기

    

<pre class="prettyprint">model = tf.keras.models.Sequential()
model.add(
   tf.keras.layers.Dense(
       units=50,
       input_dim = X_train_center.shape[1],
       kernel_initializer ='glorot_uniform',
       bias_initializer= 'zeros',
       activation='tanh'
   ))</pre>


</li>
</ul>
<p>
kernel_initializer='glorot_uniform' 글로럿 초기화,세이비어(Xavier) 심층안정망을 안정적으로 초기화, 절편은 0으로 초기화 , 케라스의 기본값
</p>
<p>
activation='softmax'  소프트 맥스는 간적적인  argMax함수이다
</p>
<p>
각 클래스의 확률을 반환함으로 다중 클래스 환경에서 의미있는 클래스 확률을 계산 할 수 있다.
</p>
<ul>

<li>옵티마이저

    

<pre class="prettyprint">sgd_optimizer = tf.keras.optimizers.SGD(
               lr=0.001,decay=1e-7,momentum=.9)
model.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy')</pre>


</li>
</ul>
<ul>

<li>훈련
</li>
</ul>



<pre class="prettyprint">history = model.fit(X_train_center,y_train_onehot,
                   batch_size=64,epochs=50,
                   verbose = 1,
                   validation_split=0.1)</pre>


<ul>

<li>예측

    

<pre class="prettyprint">y_train_pred = model.predict_classes(X_train_center,verbose=00)</pre>


</li>
</ul>
<p>
시그모이드 함수의 경우 : 함수가 0에 가까운 출력을 내면 신경망이 매우 느리게 학습하게된다. 지역 최소값에 갇힐 가능성이 높다 이런 이유로 하이퍼볼릭 탄젠트 함수를 선호
</p>
<p>
glorot_uniform
</p>
<h4>텐서 </h4>


<ul>

<li>랭크 (Rank): 텐서의 차원

<li>get_shape() 메서드는  TensorShape객체를 반환

<li>TensorShape 객체의 인덱스를 참조하면 Dimension 를 리턴함.

<li>텐서플로우는 계산 그래프를 만들지 않고 텐서를 계산 할 수 있다.

<li>
</li>
</ul>
<ul>

<li>@tf.function : tf.function decorator 
<ul>
 
<li>tf.cond, tf.case, tf.while  같은 control flow를 tf.session()을 실행할 필요가 없음
 
<li>function앞에  tf.function 데코레이터만 붙이면 computational graph를 생성해주어 바로 사용하게 준다 @
 
<li>autograph: tf.function 안의 연산은 자동으로 텐서플로 그래프에 포함되어 실행
</li> 
</ul>

<li> v1.0 vs v20 
<ul>
 
<li>2.0이 나오기 전 소스는 내부적으로는 tf.Session()을 이용하여 수행이 되고,
 
<li>오른쪽의 tensorflow 2.0소스는 내부적으로도 eager execution으로 실행이 됩니다
</li> 
</ul>

<li><code><a href="https://www.tensorflow.org/api_docs/python/tf/function">tf.function</a></code>을 함수에 붙여줄 경우, 여전히 다른 일반 함수들처럼 사용할 수 있습니다. 하지만 그래프 내에서 컴파일 되었을 때는 더 빠르게 실행하고, GPU나 TPU를 사용해서 작동하고, 세이브드모델(SavedModel)로 내보내는 것이 가능해집니다.

    

<pre class="prettyprint">@tf.function
def simple_func():
   a = tf.constant(1)
   b = tf.constant(2)
   c = tf.constant(3)

   z = 2*(a-b)+c
   return z</pre>


<ul>

<li>

<li>tf.Tensor(1, shape=(), dtype=int32)

<li>텐서플로우 변수 
<ul>
 
<li>tf.Variable(<initial-value>,name=optional-name)
 
<li>tf.Variable  에는 shape나 dtype를 설정할 수 없다.
 
<li>tf 2.x에서는 파이썬 객체를 공유하듯이 변수 공유
 
<li>tf 2.x 에서 변수값을 증가시키려면 assign() 메소드를 반복해서 전달하면 됨

        

<pre class="prettyprint">w2 = tf.Variable(np.array([[1,2,3,4],
                            [5,6,7,8]]),name='w2')
w2.assign(w2+1)
w2.assign(w2+1)</pre>

 
<ul>
 
<li>
</li> 
</ul>
</li> 
</ul>
</li> 
</ul>
</li> 
</ul>
<h4>모델만드는 법 </h4>

<ul>
<li>방법1)
   

<pre class="prettyprint">from tensorflow.keras import Model,Input
model = tf.keras.Sequential()
input = tf.keras.Input(shape=(1,))  # 튜플크기를 지정하여 input객체를 만듦.
output = tf.keras.layers.Dense(1)(input</pre>


<ul>

<li>방법2

    

<pre class="prettyprint">dense = tf.keras.layers.Dense(1)
output= dense(input)</pre>


<ul>

<li>방법3

    

<pre class="prettyprint">dense = tf.keras.layers.Dense(1)
output= dense.__call__(input)
model = tf.keras.Model(input,output)</pre>


</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>

<li>훈련
</li>
</ul>



<pre class="prettyprint">model.compile(optimizer='sgd',loss='mse')
history = model.fit(x_train,y_train,epochs=500, validation_split=0.3)
</pre>


<p>
평가 & 예측
</p>



<pre class="prettyprint">model.evaluate(x_test,y_test)
x_arr = np.arange(-2,2,0.1)
y_arr = model.predict(x_arr)</pre>


<p>
\
</p>
<ul>

<li>가중치 저장

    

<pre class="prettyprint">model = tf.keras.Model(input,output)
model.summary()
model.compile(optimizer='sgd',loss='mse')
history = model.fit(x_train,y_train,epochs=500, validation_split=0.3)

# 모델 가중치 적용
model.save_weights('simple_weight.h5')</pre>


</li>
</ul>
<ul>

<li>모델 저장

    

<pre class="prettyprint">model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(units=1,input_dim=1))
model.compile(optimizer='sgd',loss='mse')
model.load_weights('simple_weight.h5')
model.evaluate(x_test,y_test)
model.save('simple_model.h5')</pre>


<ul>

<li>모델 로딩 사용 

    

<pre class="prettyprint">model = tf.keras.models.load_model('simple_model.h5')
model.load_weights('my_model.h5')
model.evaluate(x_test,y_test)</pre>


</li>
</ul>
</li>
</ul>
<ul>

<li>modelCheckpoint 
</li>
</ul>
<p>
modelCheckpoint callback를 사용하여 최고의 가중치를 저장 할 수 있다.
</p>
<p>
EarlyStopping : 더 이상 성능이 개선되지 않을때
</p>



<pre class="prettyprint">model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(units=1,input_dim=1))
model.compile(optimizer='sgd',loss='mse')
callback_list = [tf.keras.callbacks.ModelCheckpoint(filepath='my_model.h5',
                                   monitor='val_loss',save_best_only=True),
                                   tf.keras.callbacks.EarlyStopping(patience=5)]
history= model.fit(x_train,y_train,epochs=500,
                   validation_split=0.2,callbacks=callback_list)</pre>


<p>
<a href="https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.plot.html">https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.plot.html</a>
</p>
<p>

</p>
<h5>텐서 보드 실행 </h5>

<ul>

<li>command 창에서 실행
</li>
</ul>
<p>
tensorboard --logdir=logs
</p>
<ul>

<li>jupyter notebook에서 실행

    

<pre class="prettyprint">%load_ext tensorboard
%tensorboard --logdir logs --port 6006</pre>


</li>
</ul>



<pre class="prettyprint">To reload it, use:
  %reload_ext tensorboard
Reusing TensorBoard on port 6006 (pid 77859), started 0:00:23 ago. (Use '!kill 77859' to kill it.)</pre>


<p>
케라스 층의 그래프 그리기
</p>



<pre class="prettyprint">tf.keras.utils.plot_model(model,show_shapes=True,to_file="mode1.png")</pre>


<p>
show_shapes=True 입력,출력 크기도 함께 나타내줌
</p>
<h4>15장 심층합성곱으로 신경망으로 이미지 분류 </h4>


<ul>

<li>구성요소 
<ul>
 
<li>얀 르쿤(Yann LeGun) : 손글씨 분류하는 새로운 신경망 구조 발표
 
<li>에지나 동그라미 같은 저수준 특성이 앞 쪽에서 추출되고 이 특성이 결되어 고수준 특성(건물,자동차,강아지) 같은 것 특성을 형성한다.
 
<li>Feature Map: CNN는 입력이미지에서 특성맵을 만든다. 이 맵의 각 원소는 입력 이미지의 국부적인 픽셀 패치에서 유도됨  
<ol>
  
<li>희소 연결 : 특성맵에 있는 하나의 원소는 작은 픽셀 매치 하나에만 연결
  
<li>파라미터 공유: 동일한 가중치가 입력 이미지의 모든 패치에 사용됨
  
<li>네트워크의 가중치(파라미터 갯수)가 극적으로 감소하고 중요한 특징을 잡아내는 능력이 향상됨.
</li>  
</ol>
 
<li>Conv 층 + Pooling층(subsampling) + FC 층( 다층 퍼셉트론)  
<ol>
  
<li>풀링층은 학습파라미터 없고 가중치나 절편 유닛이 없다.
</li>  
</ol>
</li>  
</ul>

<li>합성곱 출력의 크기 계산 
<ul>
 
<li>

<p id="gdcalert2" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/-1.gif). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert3">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/-1.gif" width="" alt="alt_text" title="image_tooltip">

 
<li>n : 입력벡터  p : 페딩  m: 필터의 크기  s:스트라이드 
 
<li>합성곱 연산자 ⊙
 
<li>최근에 푸리에 변환을 사용하여 합성곱을 계산.
</li> 
</ul>

<li>서브 샘플링 
<ul>
 
<li>maxing pooling
 
<li>average polling
</li> 
</ul>

<li>드롭아웃

<li>데이타 전처리 
<ul>
 
<li>training set , validation set  분리
 
<li>표준편차 구함
 
<li>28*28 형태로 만듦
 
<li>결과 training
 
<li>to_categorical: one-hot encoding 으로 변경 
</li> 
</ul>

<li>keras.API로 CNN구성

   <code>
      <pre class="prettyprint">from tensorflow.keras import layers,models
         model = models.Sequential()
         model.add(layers.Conv2D(32,(5,5),padding='valid',activation='relu', input_shape=(28,28,1)))
         #32 필터의 갯수   (5,5) 필터의 크기
         #padding = valid 기본값 :  same 구성요소
         #stride: 정수 두개로 이루어진 튜블
         #kernel_initializer : 세이비어 초기화 방식 (글로럿) : glorot_uniform
         model.add(layers.MaxPool2D((2,2)))
         # 두번째 합성곱 층
         model.add(layers.Conv2D(64,(5,5),padding='valid',activation='relu'))
         model.add(layers.MaxPool2D((2,2)))
         model.add(layers.Flatten())
         # 1개 이상의 완전 연결층으로 연결됨
         model.add(layers.Dense(1024, activation='relu'))  # 기본값 kernel_initialize: gloror_uniform  bias_init: zeros
         model.add(layers.Dropout(0.5))  # 유닛을 끌 확률 매개변수
         model.add(layers.Dense(10,activation='softmax'))
         model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['acc'])
         import time
         from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
         callback_list = [ModelCheckpoint(filepath='cnn_checkpoint.h5', monitor='val_loss',save_best_only=True),
                                        TensorBoard(log_dir="logs/{}".format(time.asctime()))]
         v_epoch = 50                               
         <ul>
         history = model.fit(X_train_centered,y_train_onehot,
                            batch_size=64, epochs=v_epoch,
                            validation_data=(X_valid_centered,y_valid_onehot),
                            callbacks= callback_list)</pre>
         
         
         </li>
         </ul>

   </code>



<li>Conv2D(32,(5,5),padding, activation. input_shape..)

<li>MaxPooling2D

<li>Conv2D(64,(5,5),padding,activation

<li>MaxPooling2D

<li>Flatten

<li>Dense(1012,activation)

<li>Dropout(0.5)

<li>Dense(10,activation=softmax

<li>compile(loss=’catergorical_crossentropy’,optimizer=’adam’

<li><strong>fit( x_train,y_train,batch_size, epoch, validation_data, callback_list)</strong>

<li>체크포인트 콜백: 
<ul>
 
<li>ModelCheckPoint callback=: vla_loss, 가중치 저장,
 
<li>Tensorboard callback  
</li> 
</ul>
</li> 
</ul>
<h5>loss 함수 시각화</h5>



<pre class="prettyprint">import matplotlib.pyplot as plt
epochs = np.arange(1,v_epoch+1)ç
plt.plot(epochs,history.history['loss'])
plt.plot(epochs,history.history['val_loss'])
plt.xlabel('epochs')
plt.ylabel('loss')
plt.show()</pre>


<h5>정확도 시각화</h5>





<pre class="prettyprint">history.history['val_acc'] 
</pre>


<h5>feature map, filter 시각화</h5>

